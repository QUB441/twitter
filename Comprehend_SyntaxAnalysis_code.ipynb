{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (only need to run once in our notebook instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyathena in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.10.3)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.12.24)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (6.1.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (0.18.2)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.15.24)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.3.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tenacity>=4.1.0->pyathena) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (2.7.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (1.23)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (0.14)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2018.7.23)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "from my_utils import language_dict, text_clean_alphanum, \\\n",
    "                    det_tweet_language, translate_tweet, \\\n",
    "                    athena_cursor\n",
    "import seaborn as sns\n",
    "\n",
    "#the below are needed to connect to the athena interface\n",
    "from pyathena.util import as_pandas\n",
    "from pyathena import connect\n",
    "\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax Analysis (part of speech tagging using Amazon Comprehend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load df from athena (can also use stored .csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39513 entries, 0 to 39512\n",
      "Data columns (total 17 columns):\n",
      "tweet_id              39513 non-null object\n",
      "user_id               39513 non-null object\n",
      "created_at            39513 non-null object\n",
      "created_at_date       39513 non-null object\n",
      "source                39513 non-null object\n",
      "truncated             39513 non-null bool\n",
      "text                  39513 non-null object\n",
      "coordinates           109 non-null object\n",
      "place_type            39513 non-null object\n",
      "place_name            39513 non-null object\n",
      "place_full_name       39513 non-null object\n",
      "place_country         39513 non-null object\n",
      "place_country_code    39513 non-null object\n",
      "reply_count           39513 non-null int64\n",
      "retweet_count         39513 non-null int64\n",
      "favorite_count        39513 non-null int64\n",
      "hashtag               39513 non-null object\n",
      "dtypes: bool(1), int64(3), object(13)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "cursor = connect(aws_access_key_id='access_key',\n",
    "                 aws_secret_access_key='secret',\n",
    "                 s3_staging_dir='s3://tweet-general/',\n",
    "                 region_name='region').cursor()\n",
    "#in the exectue() enter your athena command to view your chosen table/view\n",
    "cursor.execute(\"SELECT * FROM coronavirus.tweet\")\n",
    "tweet_text= as_pandas(cursor)\n",
    "tweet_text.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the chosen columns from the df as the input df for the function syntax_analysis_chosen_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1246162032161951745</td>\n",
       "      <td>887055386</td>\n",
       "      <td>congrats to italian scientist andrea gambotto,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240004685219725313</td>\n",
       "      <td>2630718094</td>\n",
       "      <td>just think about this..\\nin a few months time,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240036280114860032</td>\n",
       "      <td>442446326</td>\n",
       "      <td>pump this into my veins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240036282052509709</td>\n",
       "      <td>33794617</td>\n",
       "      <td>been knew  ydqoeobwhq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240036326126301184</td>\n",
       "      <td>46295482</td>\n",
       "      <td>y’all better sign them up for  qctzonzyba onli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     user_id  \\\n",
       "0  1246162032161951745   887055386   \n",
       "1  1240004685219725313  2630718094   \n",
       "2  1240036280114860032   442446326   \n",
       "3  1240036282052509709    33794617   \n",
       "4  1240036326126301184    46295482   \n",
       "\n",
       "                                                text  \n",
       "0  congrats to italian scientist andrea gambotto,...  \n",
       "1  just think about this..\\nin a few months time,...  \n",
       "2                            pump this into my veins  \n",
       "3                              been knew  ydqoeobwhq  \n",
       "4  y’all better sign them up for  qctzonzyba onli...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_df = tweet_text[['tweet_id', 'user_id', 'text']] .copy()\n",
    "tweet_input_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to write a filepath to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "def write_pd_s3_csv(df, bucket, filepath):\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, filepath).put(Body=csv_buffer.getvalue())\n",
    "    print(\"The data is successfully written to S3 path:\", bucket+\"/\"+filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function to use Amazon Comprehend to tag the tweet text and store the chosen tag and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_tweet_df is a df including the tweet text we are analysing, stored from accessing the athena console, including 'tweet_id' and 'text' columns\n",
    "#tag is the part of speech we are interesting in storing i.e. verbs, default set to VERB\n",
    "#tag_score is the lowerbound of the partofspeech score for the tagging in comprehend, default set to 0.99\n",
    "#output_df is a df where we store the results\n",
    "#s3_bucket and the filename are your chosen bucket and filepath for the df to be saved as a .csv\n",
    "\n",
    "def syntax_analysis_chosen_tag (tweet_input_df, word_tag = \"VERB\", tag_score = 0.99):\n",
    "\n",
    "    comprehend = boto3.client(service_name='comprehend', region_name='eu-west-1')\n",
    "    #create an empty dataframe to append our results to\n",
    "    df_Syntax = pd.DataFrame(columns=['tweet_id', 'Score', 'Tag','Text'])\n",
    "\n",
    "    #loop over the rows in the cleaned tweet table and analyse the syntax of the full tweet text\n",
    "    for row in range(0,len(tweet_input_df):\n",
    "        tweet_id = tweet_input_df.iloc[row]['tweet_id']\n",
    "        text = tweet_input_df.iloc[row]['text']\n",
    "        syntax_data = json.dumps(comprehend.detect_syntax(Text=text, LanguageCode='en'), sort_keys=True, indent=4)\n",
    "        #store the json data as a python dictionary\n",
    "        syntax_dic = json.loads(syntax_data)\n",
    "\n",
    "        #create a for loop to loop through i syntaxtokens and return the score, tag, text\n",
    "        for i in range(0,len(syntax_dic['SyntaxTokens'])):\n",
    "        #only store if the sytanx tag is a verb\n",
    "            if (syntax_dic['SyntaxTokens'][i]['PartOfSpeech']['Tag'] == word_tag and syntax_dic['SyntaxTokens'][i]['PartOfSpeech']['Score'] > tag_score):\n",
    "                Score = syntax_dic['SyntaxTokens'][i]['PartOfSpeech']['Score']\n",
    "                Tag = syntax_dic['SyntaxTokens'][i]['PartOfSpeech']['Tag']\n",
    "                Text = syntax_dic['SyntaxTokens'][i]['Text']\n",
    "                df_Syntax = df_Syntax.append({'tweet_id' : tweet_id, 'Score' : Score, 'Tag' : Tag, 'Text' : Text}, ignore_index = True)\n",
    "\n",
    "    return df_Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the syntax analysis run the below function with your chosen variables (defaults have been set for most - change the filename if run more than once) sotre it to a df and then export this df to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_syntax_date = syntax_analysis_chosen_tag (tweet_input_df, word_tag = \"VERB\", tag_score = 0.99)\n",
    "verb_syntax_date.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is successfully written to S3 path: virus-syntax-analysis/noun_0406.csv\n"
     ]
    }
   ],
   "source": [
    "# Write to the chosen df to S3\n",
    "s3_bucket =  'virus-syntax-analysis'  \n",
    "file_path = 'verb_date.csv'\n",
    "write_pd_s3_csv(verb_syntax_date, s3_bucket, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_syntax_date = syntax_analysis_chosen_tag (tweet_input_df, word_tag = \"NOUN\", tag_score = 0.99)\n",
    "noun_syntax_date.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to the chosen df to S3\n",
    "s3_bucket =  'virus-syntax-analysis'  \n",
    "file_path = 'noun_date.csv'\n",
    "write_pd_s3_csv(noun_syntax_date, s3_bucket, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
